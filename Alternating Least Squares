
# Importing all the necessary libraries
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from pyspark.ml import feature
from pyspark.ml import clustering
from pyspark.ml import Pipeline
from pyspark.sql import functions as fn
from pyspark.sql import SparkSession
from pyspark.ml import feature, regression, evaluation, Pipeline
from pyspark.sql import functions as fn, Row
import matplotlib.pyplot as plt
from pyspark.sql import SQLContext
from pyspark.sql.functions import col
from pyspark.ml.recommendation import ALS
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator

# Creating the Spark Session
spark = SparkSession.builder.getOrCreate()
sc = spark.sparkContext
sqlContext =SQLContext(sc)

movies_df= spark.read.csv('movie.csv', inferSchema=True, header=True)
ratings_df= spark.read.csv('rating.csv', inferSchema=True, header=True)

movies_df.show()

rate = ratings_df.select("userId",'movieId',"rating")
move = movies_df.select("movieId")
move_ratings=move.join(rate,on="movieId")

rate=rate.coalesce(1)

# Train Test Split
(training_data,test_data) = rate.randomSplit([0.8,0.2],seed=1234)
training_data.printSchema()

numerator = rate.select("rating").count()

num_users = rate.select("userId").distinct().count()
num_movies = rate.select("movieId").distinct().count()

# Sparsity Matrix
denominator = num_movies * num_users
sparsity = (1.0 - (numerator *1.0)/denominator)*100
print("The ratings dataframe is ", "%.2f" % sparsity + "% empty.")

# View the ratings dataset
rate.show()

# Filter out all userIds greater than 100
rate.filter(col("userId") < 100).show()

# Group data by userId, count ratings
rate.groupBy("userId").count().show()

# Minimum ratings for movies
print("Movie with the fewest ratings: ")
rate.groupBy("movieId").count().select(min("count")).show()

# Avg num ratings per movie
print("Avg num ratings per movie: ")
rate.groupBy("movieId").count().select(avg("count")).show()

# Minimum ratings for user
print("User with the fewest ratings: ")
rate.groupBy("userId").count().select(min("count")).show()

# Avg num ratings per users
print("Avg num ratings per user: ")
rate.groupBy("userId").count().select(avg("count")).show()

rate.printSchema()
rate= rate.select(rate.userId.cast("integer"), rate.movieId.cast("integer"), rate.rating.cast("double"))
rate.printSchema()

# Building a parameter grid to cross validate
param_grid = ParamGridBuilder() \
            .addGrid(als.rank, [50]) \
            .addGrid(als.maxIter, [50, 100]) \
            .addGrid(als.regParam, [ 0.05]) \
            .build()

# Cross Validation using parameter grid
crossval = CrossValidator(estimator=als, 
                          estimatorParamMaps=param_grid, 
                          evaluator=evaluator, 
                          numFolds=2)
                          
# Fit the cross validated model to training data
cvModel = crossval.fit(training_data)

# Make predictions on the test data
predictions = cvModel.transform(test_data)

# Accuracy metric for the predictions made
RMSE = evaluator.evaluate(predictions)
print (RMSE)

predictions.show()

user_exist=training_data.filter(training_data['userId']==11)
user_exist.show()

user_suggest = test_data.filter(training_data['userId']==11).select(['movieId', 'userId'])
user_suggest.show()

user_offer = cvModel.transform(user_suggest)
user_offer.orderBy('movieId', ascending=True).show()

# A function to recommend movies bsaed on the user id
def recommend(x):
    training_data.filter(training_data['userId']==x).select(['movieId', 'userId']).join(movies_df,on='movieId').drop('genres').show()
    user_suggest = test_data.filter(training_data['userId']==x).select(['movieId', 'userId'])
    user_offer = cvModel.transform(user_suggest)
    a=user_offer.join(movies_df,on='movieId').drop('genres').sort('prediction',ascending=False)
    print('Top movie recommendations for user are: ')
    a.show()

# Recommendation of movies for user id 12
recommend(12)
